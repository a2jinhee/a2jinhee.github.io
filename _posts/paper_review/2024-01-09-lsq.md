---
layout: minimal
title: >
  Learned Step Size Quantization
nav_exclude: true
parent: Paper Review
math: mathjax3
nav_order: 240109
---

## {{page.title}}
*{{ page.url | split: '/' | last | slice: 0, 10}}*

 <br>

**Keywords: #Quantization**

---


## 0. Abstract
- github code: [https://github.com/xuke225/EQ-Net](https://github.com/xuke225/EQ-Net)
- Proposal: EQ-Net which aims to train a robust weight-sharing quantization supernet 
    1. Elastic Quantization Space: Elastic bit-width, granularity, and symmetry → to adapt to various mainstream quantitative forms 
    2. Weight Distribution Regularization Loss (WDR-Loss) and Group Progressive Guidance Loss (GPG-Loss) → to bridge the inconsistency of the distribution for weights and ouput logits in the elastic quantization space gap 
    3. Conditional Quantization-Aware Accuracy Precision (CQAP) as an estimator to quickly search mixed-precision quantized NN in supernet. 

## 1. Introduction
- Quantization incurs added noise due to reduced precision 
    - PTQ: Only requires access to a small calibration dataset → effectiveness declines when applied to low bit quantization (<= 4bits)
    - QAT: By simulating the quantization operation during training or fine-tuning, the network can adapt to quantization noise → better than PTQ
- Problem: The forms of quantization supported by different hardware platforms are all different. (ex) NVIDIA's GPU (channel-wise symmetric quantization in TensorRT inference engine), Qualcomm's DSP (per-tensor asymmetric quantization in SNPE inference engine) → repeated optimization leads to low efficiency of model qunatization deployment
- Proposal
    1. Elastic Quantization Space: A unified quantization formula that integrates various model quantization forms and implementing elastic switching of 1) Quantization bit-width, 2) Quanization granularity 3) Quantization symmetry through parameter splitting
    2. WDR-Loss, GPG-Loss: Unlike NAS, EQ-Net is fully parameter-shared, and **there is no additional weight parameter optimization space with network structure differences.** → problem of **negative gradient suppression**(due to different quantization forms; samples with inconsistent predictions between quantization configurations) → WDR-Loss, GPG-Loss is an efficient training strategy for EQ-Net
    3. CQAP combined with a genetic algorithm: Specify any form in the elastic quantization space and quickly obtain a quantized model with the corresponding accuracy. (Can achieve both uniform and MPQ)