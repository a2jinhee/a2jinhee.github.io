---
layout: default
title: >
  DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients
nav_exclude: true
parent: Paper Review
math: mathjax3
nav_order: 240112
---

## {{page.title}}
*{{ page.url | split: '/' | last | slice: 0, 10}}*

 <br>

**Keywords: #Quantization**

---

## 0. Abstract
- Proposal: A method to train NNs that have low bitwidth weights/ activations using low bitwidth parameter gradients.  

## 1. Introduction
- BNN, XNOR-Net: Both weights and activations of conv layers are binarized. â†’ Computationally expensive convolutions can be done by **bitwise operation kernels** during forward pass.
<!-- $$
\begin{align*}
\mathbf{x} \cdot \mathbf{y}=\operatorname{bitcount}(\operatorname{and}(\mathbf{x}, \mathbf{y})), x_i, y_i \in\{0,1\} \forall i
\end{align*}
$$ --> 
- Dot product of bit vectors $x$ and $y$ using bitwise op. 
<div align="center"><img style="background: white;" src="https://latex.codecogs.com/svg.latex?%5Cbegin%7Balign*%7D%0A%5Cmathbf%7Bx%7D%20%5Ccdot%20%5Cmathbf%7By%7D%3D%5Coperatorname%7Bbitcount%7D(%5Coperatorname%7Band%7D(%5Cmathbf%7Bx%7D%2C%20%5Cmathbf%7By%7D))%2C%20x_i%2C%20y_i%20%5Cin%5C%7B0%2C1%5C%7D%20%5Cforall%20i%0A%5Cend%7Balign*%7D"></div>
- When $x$ and $y$ are vectors of ${-1, 1}$, 
<!-- $$
\begin{align*}
\mathbf{x} \cdot \mathbf{y}=N-2\operatorname{bitcount}(\operatorname{xnor}(\mathbf{x}, \mathbf{y})), x_i, y_i \in\{-1,1\} \forall i
\end{align*}
$$ --> 

<div align="center"><img style="background: white;" src="https://latex.codecogs.com/svg.latex?%5Cbegin%7Balign*%7D%0A%5Cmathbf%7Bx%7D%20%5Ccdot%20%5Cmathbf%7By%7D%3DN-2%5Coperatorname%7Bbitcount%7D(%5Coperatorname%7Bxnor%7D(%5Cmathbf%7Bx%7D%2C%20%5Cmathbf%7By%7D))%2C%20x_i%2C%20y_i%20%5Cin%5C%7B-1%2C1%5C%7D%20%5Cforall%20i%0A%5Cend%7Balign*%7D"></div> 

- No previous work has succeeded in quantizing **gradients** to numbers with bitwidth less than 8 during backward pass. 
- Contributions
  1. DoReFa-Net: Generalize the method of binarized NN, bit convolution kernels to accelerate both forward and backward pass of the training process. 
  2. Bit convolution can be implemented on CPU, FPGA, ASIC and GPU. Considerably reduces energy consumption of low bitwidth NN training
  3. Quantization sensitivity: gradients > activations > weights  

## 2. DoReFa-Net
- A method to train NN that has low bw(bitwidth) weights/activation with low bw parameter gradients. 
- While weights/activations can be deterministically quantized, gradients need to be **stochastically quantized**.

### 2.1 Using Bit Convolution Kernels in Low Bitwidth NN
![](/img/2024-01-16-14-14-59.png){: width="90%"}{: .center-img}  
- Computation complexity: $O(MK)$

### 2.2 Straight-Through Estimator 
![](/img/2024-01-16-14-39-28.png){: width="90%"}{: .center-img}

### 2.3 Low Bitwidth Quantization of Weights
