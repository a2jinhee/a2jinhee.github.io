---
layout: minimal
title: >
  EQ-Net: Elastic Quantization Neural Networks
nav_exclude: true
parent: Paper Review
math: mathjax3
---

## EQ-Net: Elastic Quantization Neural Networks

_2022.12.17_  
 <br>

**Keywords: #Quantization**

---


## 0. Abstract
- github code: [https://github.com/xuke225/EQ-Net](https://github.com/xuke225/EQ-Net)
- Proposal: EQ-Net which aims to train a robust weight-sharing quantization supernet 
    1. Elastic Quantization Space: Elastic bit-width, granularity, and symmetry → to adapt to various mainstream quantitative forms 
    2. Weight Distribution Regularization Loss (WDR-Loss) and Group Progressive Guidance Loss (GPG-Loss) → to bridge the inconsistency of the distribution for weights and ouput logits in the elastic quantization space gap 
    3. Conditional Quantization-Aware Accuracy Precision (CQAP) as an estimator to quickly search mixed-precision quantized NN in supernet. 

## 1. Introduction
- Quantization incurs added noise due to reduced precision 
    - PTQ: Only requires access to a small calibration dataset → effectiveness declines when applied to low bit quantization (<= 4bits)
    - QAT: By simulating the quantization operation during training or fine-tuning, the network can adapt to quantization noise → better than PTQ
- Problem: The forms of quantization supported by different hardware platforms are all different. (ex) NVIDIA's GPU (channel-wise symmetric quantization in TensorRT inference engine), Qualcomm's DSP (per-tensor asymmetric quantization in SNPE inference engine) → repeated optimization leads to low efficiency of model qunatization deployment
- Proposal
    1. Elastic Quantization Space: A unified quantization formula that integrates various model quantization forms and implementing elastic switching of 1) Quantization bit-width, 2) Quanization granularity 3) Quantization symmetry through parameter splitting
    2. WDR-Loss, GPG-Loss: Unlike NAS, EQ-Net is fully parameter-shared, and **there is no additional weight parameter optimization space with network structure differences.** → problem of **negative gradient suppression**(due to different quantization forms; samples with inconsistent predictions between quantization configurations) → WDR-Loss, GPG-Loss is an efficient training strategy for EQ-Net
    3. CQAP combined with a genetic algorithm: Specify any form in the elastic quantization space and quickly obtain a quantized model with the corresponding accuracy. (Can achieve both uniform and MPQ)

## 2. Related Work
- One-Shot Network Architecture Search 
- Multi-Bit Quantization of Neural Networks

## 3. Approach
### 3.1 Quantization Preliminaries
- Quantization / Dequantization operation    
<!-- $$
\begin{align*}
\hat{w} &= \text{clip}([\frac{w}{s}]+z, -2^{b-1}, 2^{b-1}-1) \\
\bar{w} &= s \cdot (\hat{w}-z)
\end{align*}
$$ --> 

<div align="center"><img style="background: white;" src="https://latex.codecogs.com/svg.latex?%5Cbegin%7Balign*%7D%0A%5Chat%7Bw%7D%20%26%3D%20%5Ctext%7Bclip%7D(%5B%5Cfrac%7Bw%7D%7Bs%7D%5D%2Bz%2C%20-2%5E%7Bb-1%7D%2C%202%5E%7Bb-1%7D-1)%20%5C%5C%0A%5Cbar%7Bw%7D%20%26%3D%20s%20%5Ccdot%20(%5Chat%7Bw%7D-z)%0A%5Cend%7Balign*%7D"></div> 


### 3.2 Elastic Quantization Space Design 
#### 3.2.1 Elastic Quantization Bit-Width
- Separate and store the quantization step size and zero-point required for different quantization bit-widths.
- Higher bit-widths: Small quantization step size → Large saturation truncation range 
- Lower bit-widths: Larger quantization step size → Small saturation truncation range
→ Alleviates training pressure for hyperparameters
→ Poses challenges to the robustness of shared weights.

#### 3.2.2 Elastic Quantization Symmetry
- Symmetric quantization: Zero-point is fixed to 0 (z = 0)
- Asymmetric quantization: Zero-point is adjustable to different ranges (z ∈ Z).
- The switching between the two is achieved by dynamically modifying the value of the zero point.

#### 3.2.1 Elastic Quantization Granularity
- Supports both **per-tensor**(one set of step size and zero-point for a tensor in one layer) and **per-channel**(quantizes each weight kernel independently) quantization.
- **per-tensor** > **per-channel**
- Step size and zero point for per-tensor can be obtained heuristically from per-channel, or can be learned as independent parameters
- EQG is for weights only, activation is are all in the form of per-tensor


### 3.3 Elastic Quantization Network Modeling _
- Elastic quantization space of a model: $\varepsilon = \\{\varepsilon_{b}, \varepsilon_{g}, \varepsilon_{s}\\}$
- Training objective: Minimize the task loss under all elastic spaces elastic quantization spaces by optimizing the **weights, step sizes, and zero points**
  
<!-- $$
\min_{w^*, s^*, z^*} \sum_\varepsilon \mathcal{L}_\text{val}(\text{QNN}(\hat w, \hat x, s, z))
$$ --> 

<div align="center"><img style="background: white;" src="https://latex.codecogs.com/svg.latex?%5Cmin_%7Bw%5E*%2C%20s%5E*%2C%20z%5E*%7D%20%5Csum_%5Cvarepsilon%20%5Cmathcal%7BL%7D_%5Ctext%7Bval%7D(%5Ctext%7BQNN%7D(%5Chat%20w%2C%20%5Chat%20x%2C%20s%2C%20z))"></div> 


### 3.4 Elastic Quantization Training
#### Weight Distribution Regularization
- DNN weights often conform to Gaussian or Laplace distributions
- Skewness regularization: Reducing skewness → enhance robustness of weights in elastic **symmetry**
- Kurtosis regularization: Reducing sharpness → enhance robustness of weights in elastic **bit-width**  

<!-- $$
\begin{align*}
\text{Skew}[w] &= E\bigl[(\frac{w-\mu}{\sigma})^3\bigr] \\
\text{Kurt}[w] &= E\bigl[(\frac{w-\mu}{\sigma})^4\bigr]
\end{align*}
$$ --> 

<div align="center"><img style="background: white;" src="https://latex.codecogs.com/svg.latex?%5Cbegin%7Balign*%7D%0A%5Ctext%7BSkew%7D%5Bw%5D%20%26%3D%20E%5Cbigl%5B(%5Cfrac%7Bw-%5Cmu%7D%7B%5Csigma%7D)%5E3%5Cbigr%5D%20%5C%5C%0A%5Ctext%7BKurt%7D%5Bw%5D%20%26%3D%20E%5Cbigl%5B(%5Cfrac%7Bw-%5Cmu%7D%7B%5Csigma%7D)%5E4%5Cbigr%5D%0A%5Cend%7Balign*%7D"></div>

#### Group Progressive Guidance 


---


### 3.3 Training 

#### 3.3.1 Quantization-aware training (QAT)
**"Train the model taking quantization into consideration"**
- A full precision copy of the weights W is maintained throught the training. 
- Small gradients are accumulated without loss of precision → gradients effect the 'full precision copy of weights' 
- Once the model is trained, only the quantized weights are used for inference. 
- **BN layer is important in low-precision DNN** 
- $y'$ is first passed into the BN layer and THEN quantized into $y_Q$
    
#### 3.3.2 Weights 

- **Uniform quantization strategy**: 
    - Normalize weights to [0,1]:  
    [graph](https://www.desmos.com/calculator/1du8fwsskp)  
    <!-- $$
    w' = \frac{tanh(w)}{2\max(|tanh(w)|)}+0.5 = \frac{1}{2}*(\frac{tanh(w)}{\max(|tanh(w)|)}+1)
    $$ --> 
    <div align="center"><img style="background: white;" src="https://latex.codecogs.com/svg.latex?%20%20%20%20w'%20%3D%20%5Cfrac%7Btanh(w)%7D%7B2%5Cmax(%7Ctanh(w)%7C)%7D%2B0.5%20%3D%20%5Cfrac%7B1%7D%7B2%7D*(%5Cfrac%7Btanh(w)%7D%7B%5Cmax(%7Ctanh(w)%7C)%7D%2B1)"></div> 
    Where $\tanh(w)$ limits the range of weights to [-1,1] (normalize before quantization), and $\max|\tanh(w)|$ is the maximum taken over all weights in the layer. By construction, $\tanh(w)/\max|\tanh(w)|$ is still in the range of [-1,1]. Add 1 and half the value to keep the weights in a nice [0,1] range in order to proceed with the scaling step. (▲)
    - Quantize normalized value into N-bit integer     
    <!-- $$
    w_Q' = \text{INT}(round(w'* \text{MAX}_N)) \quad s' = 1/\text{MAX}_N
    $$ --> 
    <div align="center"><img style="background: white;" src="https://latex.codecogs.com/svg.latex?%20%20%20%20w_Q'%20%3D%20%5Ctext%7BINT%7D(round(w'*%20%5Ctext%7BMAX%7D_N))%20%5Cquad%20s'%20%3D%201%2F%5Ctext%7BMAX%7D_N"></div>  
    
    Where $\text{MAX}_N$ denotes the upper-bound of N-bit integer. Because $w'$ is in range [0,1], we only need to multiply $\text{MAX}_N$ and round it to an integer to get $w_Q'$. 
    - Values remapped to approximate the range of floating point values to obtain  

    <!-- $$
    w_Q = 2*w_Q'-1 \quad s = \mathbb{E}(|w|)/MAX_N
    $$ --> 
    <div align="center"><img style="background: white;" src="https://latex.codecogs.com/svg.latex?%20%20%20%20%20%20%20%20w_Q%20%3D%202*w_Q'-1%20%5Cquad%20s%20%3D%20%5Cmathbb%7BE%7D(%7Cw%7C)%2FMAX_N"></div> 
    Do the reverse op. of (▲) so that $w_Q$ becomes the intended quantization. The rationale of multiplying expectation of the weights comes from XNOR-Net. 
    - Approximate $w$ with $s*w_Q$
    Keeping track of how $w$ was scaled throughout the quantization process,multiply $s$ to the final quantized weight. 

- Forward Pass
    - Execute feed-forward pass with quantized weights:  

    <!-- $$
    y' = s*(w_Q \cdot x_Q) + b
    $$ --> 
    <div align="center"><img style="background: white;" src="https://latex.codecogs.com/svg.latex?%20%20%20%20%20%20%20%20y'%20%3D%20s*(w_Q%20%5Ccdot%20x_Q)%20%2B%20b"></div>
    Overload of notation $s$ - (1) quantization scaling factor for weights, (2) layer-wise scaling factor to reduce output range variation 

- Backward Pass
    - Gradients are computed w.r.t the underlying float-value variable $w$(because we use the quantized version $w_Q$, in the feed-forward pass) and updates are applied to $w$ as well.
    - **Straight through estimator (STE)**: Used to approximate gradients   
        (ex) round op. has zero derivatives everywhere (why? because it's a step function). With STE, the gradient of round op. is 1. 

#### 3.3.3 Activations 
- Clipping: Clip acitvation values to be within [0,1]

#### 3.3.4 Dynamic Model-wise Quantization 
- Previous works: Bit-width N is fixed during the training process → In runtime if we alter N, accuracy drops drastically 
- **Proposal: Dynamically change N within the training stage to align the training and inference process**
- Problem 1: Distribution of activations varies under different bit-width N, especially when N is small, failing to converge 
- Solution 1: Adopt BN layer that calculates running average
- Problem 2: Still fails
- Solution 2: (Proposal) Adopt **dynamically changed BN layer** to work with different N in training. Parameters of all BatchNorm layers are kept after training and used in inference. Storage for each bitwidth BN layer is negligible. 


#### 3.3.5 Etc. 
- Knowledge distillation: KD gives ~$\pm$1% boost

## 4. Experiments
### 4.1 Implementation Details
- For all models, first and last layer is real-valued. 
- In training, we train the networks with bit-width candidates {1,2,4,8,32}

### 4.2 Comparison to Dedicated Models 
- The proposed any-precision DNN generally achieved comparable performance to the competitive dedicated models. 
- Any-precision model is more compact

### 4.3 Post-Training Quantization Methods 
- Compared with three alternative post-training quantization method
    1. Directly quantizes dedicated models with bit-shifting (truncation)
        - Simply drop LSB. 
        - Fails dramatically. 
    2. Bit-shifting + BatchNorm calibration process
        - Calibration process: BN statistics is recalculated by feed-forwarding a number of training samples
        - Worked in 4-bit, but failed in 1, 2 bit settings
    3. ACIQ
        - Analytical weight clipping, adaptive bit allocation, bias correction
    4. (Proposed Method) Calibrate the remaining 3, 5, 6, 7-bit settings to get the missed copies of BN layer params. 
        - In runtime we can freely choose any precision level from 1 to 8 bits. 

### 4.4 Dynamically Change BN Layers
- Keeping multiple copies of BN layer params for different bit-widths
    → Minimized input variations to the convolutional layers
    → Allow same set of convolutional layer params. to support any-precision in run time
![](/img/2023-11-25-01-31-57.png){: width="50%"}{: .center-img}

### 4.5 Ablation Studies
#### Candidate bit-width List 
How does the candidate bit-width list used during training influence test performance on other bit-widths
- Training with more candidate bit-width generally leads to better generalization to the others.
    - i.e. 1,2,4,8-bits combination is better than rest. 
- Coverage of candidate bit-width matters.
    - i.e. 1,8-bits combination performs more stable across different runtime bit-width compared with 2,8-bits, 4,8-bits combination  

#### Knowledge Distillation